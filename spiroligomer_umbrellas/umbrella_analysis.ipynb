{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /media/matt/ext/projects/spiroligomers/brown\n",
      "\u001b[0m\u001b[01;34mPROJ14101\u001b[0m/  \u001b[01;32mtram.ipynb\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import glob, itertools, os, subprocess, re\n",
    "import sys, time, tqdm, itertools, socket\n",
    "import mdtraj as md\n",
    "import msmbuilder.utils\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import groupby, count\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "# Script: matplotlib.use('Agg')  | Notebook: %matplotlib inline\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from msmbuilder.cluster import KCenters, KMeans, KMedoids\n",
    "from msmbuilder.decomposition import tICA\n",
    "from msmbuilder.featurizer import AtomPairsFeaturizer\n",
    "from msmbuilder.msm import ContinuousTimeMSM, implied_timescales, MarkovStateModel\n",
    "from operator import itemgetter\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pymbar import MBAR\n",
    "from pyemma.thermo import mbar\n",
    "\n",
    "'''This script has lots of functionality and is based on analyzing Gromacs trajectories. A list of trajectory\n",
    "   files is given as trajectory_files, as well as a general structure file. Other structure files should contain\n",
    "   the same name as the corresponding trajectory file, e.g. traj_001.trr traj_001.gro.\n",
    "'''\n",
    "\n",
    "# this project represents a spiroligomer (1) from https://doi.org/10.1371/journal.pone.0045948\n",
    "# bound to MDM2 (PDB: 1ycr)\n",
    "# these runs represent the 20 ensembles: lam = false, d = {0.1, 2.0, 0.1}\n",
    "\n",
    "# featurization parameters\n",
    "project_title = 'PROJ14101' # creates sub-directory\n",
    "structure_file = '%s/xtc.gro'%project_title\n",
    "runs = 20\n",
    "clones = 50\n",
    "run_indices = range(runs)\n",
    "equil_time = 1. # ns\n",
    "custom_residues = ['B1A','B1B','B2A','B2B','B2C','B2D','B2E','B3A','B3B']\n",
    "custom_residues += ['B4A','B4B','B4C','B4D','B5A','B5B','B5C','B6A'] # for spiroligomers\n",
    "custom_residues += ['1MQ','20Q','20U','I18','I31','K23','NUT','YIN'] # for nutlins\n",
    "\n",
    "# tICA parameters\n",
    "tica_lagtime = 10 # determine from implied timescales / GMRQ\n",
    "n_components = 8 # how many tICs to compute\n",
    "n_clusters = 100 # denotes number of microstates\n",
    "n_timescales = n_components # plot all eigenvalues --> timescales\n",
    "md_time_step = 0.02 # ns\n",
    "subsampled_time_step = 1. # ns multiplier of timescales and lagtimes in implied timescale plot\n",
    "stride = int(subsampled_time_step / md_time_step) # time step stride for sub-sampling\n",
    "equil_steps = int(equil_time / md_time_step) # time steps to be removed from start\n",
    "lagtimes = np.array([1,2,4,8,16,32,64,128,256,512,1024]) # log scale\n",
    "cluster_method = 'kcenters' # 'kcenters/kmeans/kmedoids'\n",
    "all_ticas = list(itertools.permutations(range(1,n_components+1), 2)) # all combinations\n",
    "all_ticas = [[1,2]] # override: just show analysis for first two components\n",
    "cluster_percentage_cutoff = n_clusters/64 # clusters with a relative population less than this\n",
    "                              # number will not be labeled on plot i.e. 0 : all clusters labeled\n",
    "    \n",
    "# MBAR parameters\n",
    "K = len(run_indices) # sets n_ensembles = n_runs\n",
    "L = K # secondary ensemble count for MBAR\n",
    "kspring_k = [200.0] * K # spring constant for each ensemble in kJ/mol/nm^2\n",
    "equilibrium_distances = [0.1+0.1*k for k in range(K)]\n",
    "protein_anchor = 555\n",
    "restrained_distance_indices = [[[555,'COM']]]*K # which distances are restrained for each\n",
    "restrained_distance_labels = ['d555-ligand_COM']*K # restraint labels for plots\n",
    "temperature = 300.0 # in K\n",
    "T_k = np.ones(K,float)*temperature # This simulation uses only 300K sampling\n",
    "kB = 1.381e-23 * 6.022e23 / 1000.0 # Boltzmann constant in kJ/mol/K\n",
    "beta = 1.0 / (kB * temperature) # inverse temperature of simulations (in 1/(kJ/mol))\n",
    "\n",
    "print('Current Directory: %s'%os.getcwd())\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances():\n",
    "    \n",
    "    \"\"\"Calculate pair-wise distance features and save as .npy files. Index selection is done within the function.\n",
    "       Feature labels are returned to match tICA components back to the features that make them up.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nCalculating distances...\")\n",
    "    \n",
    "    if not os.path.exists(project_title + '/features'):\n",
    "        os.mkdir(project_title + '/features')\n",
    "        \n",
    "    protein_residues = range(50,64) # residue indices for MDM2 binding helix\n",
    "    trajectory_files = []\n",
    "    \n",
    "    for run in range(runs):\n",
    "        trajectory_files += sorted(glob.glob('%s/traj_data/RUN%d/*xtc'%(project_title,run)))\n",
    "\n",
    "    for i in range(len(trajectory_files)): # For each trajectory file\n",
    "        traj = md.load(trajectory_files[i], top=structure_file)\n",
    "        print(\"Loaded \" + trajectory_files[i] + \" with top: \" + structure_file)\n",
    "\n",
    "        custom_residues = ['1MQ','20Q','20U','I18','I31','NUT','YIN','K23']\n",
    "        custom_residues += ['B1A','B1B','B2A','B2B','B2C','B2D','B2E','B3A','B3B']\n",
    "        custom_residues += ['B4A','B4B','B4C','B4D','B5A','B5B','B5C','B6A']\n",
    "\n",
    "        protein_indices = [ a.index for a in traj.topology.atoms if a.name in ['CA'] and a.residue.index in protein_residues and a.residue.name not in custom_residues]\n",
    "        ligand_indices = [ a.index for a in traj.topology.atoms if a.element.symbol == 'C' and a.residue.name in custom_residues]\n",
    "\n",
    "        # Transform indices into distances and save\n",
    "        pairs = list(itertools.product(protein_indices[::2], ligand_indices[::4]))\n",
    "        \n",
    "        if len(pairs) > len(traj)/stride:\n",
    "            print(\"Number of features (%d) exceeds strided trajectory length (%d). Skipping.\\n\" %(len(pairs),len(traj)/stride))\n",
    "            continue\n",
    "        \n",
    "        feature_labels = [[[str(traj.topology.atom(j[0]).residue.index) +\n",
    "                            traj.topology.atom(j[0]).residue.name,traj.topology.atom(j[0]).name],\n",
    "                          [str(traj.topology.atom(j[1]).residue.index) + \n",
    "                           traj.topology.atom(j[1]).residue.name,traj.topology.atom(j[1]).name]] for j in pairs]\n",
    "        \n",
    "        features = AtomPairsFeaturizer(pairs)\n",
    "        transformed_data = features.fit_transform(traj[equil_steps:][::stride])\n",
    "        \n",
    "        for j in range(len(transformed_data)):\n",
    "            transformed_data[j] = transformed_data[j][0]\n",
    "            \n",
    "        print(\"Saved %d pair-wise distance features over %d frames.\\n\" %(len(pairs),len(transformed_data)))\n",
    "        feature_file = re.sub('.xtc','',re.sub('traj_data/RUN.*/','features/',trajectory_files[i]))\n",
    "        np.save(feature_file, transformed_data)\n",
    "        \n",
    "    return feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center_of_mass(traj, atom_indices=None):\n",
    "    \"\"\"\n",
    "    Compute the center of mass for each frame.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    traj : Trajectory\n",
    "        Trajectory to compute center of mass for\n",
    "    atom_indices : list of int\n",
    "        Atoms to compute center of mass for. If None,\n",
    "        will compute over all atoms\n",
    "    Returns\n",
    "    -------\n",
    "    com : np.ndarray, shape=(n_frames, 3)\n",
    "         Coordinates of the center of mass for each frame\n",
    "    \"\"\"\n",
    "\n",
    "    if atom_indices is None:\n",
    "        atoms = traj.top.atoms\n",
    "        coords = traj.xyz\n",
    "    else:\n",
    "        atoms = [traj.top.atom(i) for i in atom_indices]\n",
    "        coords = np.take(traj.xyz, atom_indices, axis=1)\n",
    "\n",
    "    com = np.zeros((traj.n_frames, 3))\n",
    "    masses = np.array([a.element.mass for a in traj.top.atoms])\n",
    "    masses = np.array([a.element.mass for a in atoms])\n",
    "    masses /= masses.sum()\n",
    "\n",
    "    #for i, x in enumerate(traj.xyz):\n",
    "    for i, x in enumerate(coords):\n",
    "        com[i, :] = x.astype('float64').T.dot(masses)\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tica_components():\n",
    "          \n",
    "    '''Load in the features, calculate a given number of tICA components (tica_components) given a\n",
    "       lagtime (lag_time), and save tICA coordinates and eigenvector data. It then creates and populates\n",
    "       a list for each desired component, clusters the data, saving normalized populations as populations.dat\n",
    "       and saving each cluster center as a .pdb. tICA plots are created and saved, and implied timescales are\n",
    "       calculated, saved, and plotted.\n",
    "    '''\n",
    "          \n",
    "    print(\"\\nCalculating tICA components...\")\n",
    "    if not os.path.exists(project_title + '/tica'):\n",
    "        os.mkdir(project_title + '/tica')\n",
    "        \n",
    "    verbose = False\n",
    "    \n",
    "    # Load in feature files\n",
    "    feature_files = []\n",
    "    for i in range(runs):\n",
    "        feature_files += sorted(glob.glob(project_title + '/features/' + \"P*R%d_*npy\"%i))\n",
    "    features = [ np.load(filename) for filename in feature_files]\n",
    "    \n",
    "\n",
    "    # Perform tICA calculation\n",
    "    tica_coordinates = tICA(lag_time=tica_lagtime,\n",
    "        n_components=int(n_components)).fit_transform(features)\n",
    "    tica_components = tICA(lag_time=tica_lagtime,\n",
    "        n_components=int(n_components)).fit(features)\n",
    "          \n",
    "    np.save(project_title + '/tica/' + 'tica_coords-lag_%d-comp_%d.npy' %(tica_lagtime, n_components), tica_coordinates)\n",
    "    np.save(project_title + '/tica/' + 'tica_comps-lag_%d-comp_%d.npy' %(tica_lagtime, n_components), tica_components)\n",
    "          \n",
    "    # Extract tICA eigenvectors\n",
    "    eigenvectors = np.transpose(tica_components.eigenvectors_)\n",
    "          \n",
    "    # Initiate and populate an array for each component    \n",
    "    for i in range(n_components):\n",
    "        exec('tica_' + str(i+1) + ' = []')\n",
    "          \n",
    "    for i in tqdm.tqdm(range(len(features))):\n",
    "        for j in range(len(tica_coordinates[i])):\n",
    "            for k in range(n_components):\n",
    "                exec('tica_' + str(k+1) + '.append(tica_coordinates[i][j][k])')\n",
    "            \n",
    "    # Perform clustering based on the cluster_method parameter.\n",
    "    if cluster_method == 'kcenters':\n",
    "        print('Clustering via KCenters...')\n",
    "        clusters = KCenters(n_clusters)\n",
    "    elif cluster_method == 'kmeans':\n",
    "        print('Clustering via KMeans...')\n",
    "        clusters = KMeans(n_clusters)\n",
    "    elif cluster_method == 'kmedoids':\n",
    "        print('Clustering via KMedoids...')\n",
    "        clusters = KMedoids(n_clusters)\n",
    "    else:\n",
    "        sys.exit('Invalid cluster_method. Use kcenters/kmeans/kmedoids.')\n",
    "        \n",
    "    # Determine cluster assignment for each frame.      \n",
    "    sequences = clusters.fit_transform(tica_coordinates)\n",
    "    np.save(project_title + '/tica/' + 'lag_%d_clusters_%d_sequences.npy' %(tica_lagtime, n_clusters), sequences)\n",
    "    np.save(project_title + '/tica/' + 'lag_%d_clusters_%d_center.npy' %(tica_lagtime, n_clusters),\n",
    "        clusters.cluster_centers_)\n",
    "\n",
    "    # Determine cluster populations, normalize the counts, and save as percentages for\n",
    "    # labeling if a cluster contains more than cluster_percentage_cutoff percent of the data.\n",
    "    # Finally, save normalized counts.\n",
    "    \n",
    "    print(\"\\nDetermining cluster populations...\")\n",
    "    if not os.path.exists(project_title + '/tica/%s_clusters'%cluster_method):\n",
    "        os.mkdir(project_title + '/tica/%s_clusters'%cluster_method)\n",
    "    if not os.path.exists(project_title + '/tica/plots'):\n",
    "        os.mkdir(project_title + '/tica/plots')\n",
    "        \n",
    "    counts = np.array([len(np.where(np.concatenate(sequences)==i)[0]) for i in range(n_clusters)])\n",
    "    normalized_counts =  counts/float(counts.sum())\n",
    "    percentages = [ i*100 for i in normalized_counts ]\n",
    "    population_labels = [ [i,\"%.2f\"%percentages[i]] for i in range(len(percentages)) if percentages[i] > cluster_percentage_cutoff ]\n",
    "    np.savetxt(project_title + '/tica/%s_clusters/populations.dat'%cluster_method, normalized_counts)\n",
    "\n",
    "\n",
    "    # Plot all unique combinations of tICA components\n",
    "    print(\"\\nPlotting tICA components...\")\n",
    "    for j in tqdm.tqdm(range(len(all_ticas))): # For each pair\n",
    "        if all_ticas[j][0] < all_ticas[j][1]:\n",
    "            plt.figure(j, figsize=(20,16))\n",
    "            plt.hexbin(eval(\"tica_\"+str(all_ticas[j][0])), eval(\"tica_\"+str(all_ticas[j][1])), bins='log')\n",
    "            x_centers = [clusters.cluster_centers_[i][all_ticas[j][0]-1] for i in range(len(clusters.cluster_centers_))]\n",
    "            y_centers = [clusters.cluster_centers_[i][all_ticas[j][1]-1] for i in range(len(clusters.cluster_centers_))]\n",
    "            high_pop_x_centers = [ x_centers[i] for i in range(len(x_centers)) if percentages[i] > cluster_percentage_cutoff ]\n",
    "            high_pop_y_centers = [ y_centers[i] for i in range(len(y_centers)) if percentages[i] > cluster_percentage_cutoff ]\n",
    "            plt.plot(x_centers, y_centers, color='y', linestyle=\"\", marker=\"o\")\n",
    "            plt.plot(eval(\"tica_\"+str(all_ticas[j][0])+'[0]'), eval(\"tica_\"+str(all_ticas[j][1])+'[0]'), color='k', marker='*',markersize=24)\n",
    "            plt.xlabel('tic'+str(all_ticas[j][0]))\n",
    "            plt.ylabel('tic'+str(all_ticas[j][1]))\n",
    "            plt.title(project_title)\n",
    "            # Add labels for high-population cluster centers\n",
    "            for label, x, y in zip(population_labels, high_pop_x_centers, high_pop_y_centers):\n",
    "                plt.annotate(\n",
    "                  label,\n",
    "                  xy = (x, y), xytext = (-15, 15),\n",
    "                  textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "                  bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "                  arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "            plt.savefig(project_title + '/tica/plots/' + 'tica_'+str(all_ticas[j][0])+'_'+str(all_ticas[j][1])+'.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Calculate and save cluster entropy\n",
    "    print('\\nDetermining cluster entropy...')\n",
    "    cluster_entropy = (-kB*temperature*normalized_counts*np.log(normalized_counts)).sum()\n",
    "    print(cluster_entropy)\n",
    "    # np.savetxt(project_title + '/' + 'cluster_entropy.dat', cluster_entropy)\n",
    "\n",
    "          \n",
    "    # Write out PDBs for each cluster center\n",
    "    print(\"Performing cluster analytics and saving center PDBs...\\n\")\n",
    "\n",
    "    trajectory_files = []\n",
    "    for run in range(runs): # get only xtc files that correlate to npy files\n",
    "        trajectory_files += [re.sub('features',\n",
    "                                'traj_data/RUN%d'%run,re.sub('npy','xtc',x)\n",
    "                                 ) for x in sorted(glob.glob('%s/features/*R%d_*npy'%(project_title,run)))] \n",
    "\n",
    "    for i in range(len(features)):\n",
    "        n_snapshots = len(clusters.distances_[i])\n",
    "          \n",
    "        # Determine frames that are cluster centers\n",
    "        cluster_indices = np.arange(n_snapshots)[ (clusters.distances_[i] < 1e-6) ]\n",
    "        # Determine number of each cluster, correlates to populations.dat\n",
    "        cluster_labels = sequences[i][cluster_indices]\n",
    "\n",
    "        # Save each cluster center as a pdb\n",
    "        if list(cluster_indices): # load center-containing xtcs to check length\n",
    "            xtc_len = len(md.load(trajectory_files[i],top=structure_file))\n",
    "            \n",
    "        for j in range(len(cluster_indices)):\n",
    "            frames = range(xtc_len) # map the strided frame number back to xtc frame number\n",
    "            strided_frames = frames[equil_steps:][::stride]\n",
    "            xtc_frame = frames.index(strided_frames[cluster_indices[j]])\n",
    "            cluster_traj = md.load_frame(trajectory_files[i], xtc_frame, top=structure_file)\n",
    "            cluster_traj.save_pdb(project_title + '/tica/%s_clusters/state_%d_%.3f.pdb'%(cluster_method,\n",
    "                                cluster_labels[j],percentages[cluster_labels[j]]))\n",
    "            if verbose:\n",
    "                print('Successfully saved PDB for cluster: %d, (rel.pop: %.3f)'%(cluster_labels[j],percentages[cluster_labels[j]]))\n",
    "                print('traj_file: %s (%d/%d)'%(trajectory_files[i],i,len(features)))\n",
    "                print('frame: %d (%d/%d centers from this trajectory)'%(cluster_indices[j],j,len(cluster_indices)))\n",
    "                print('strided: npy_frame/npy_len = %d/%d = %f'%(cluster_indices[j],n_snapshots,cluster_indices[j]/n_snapshots))\n",
    "                print('re-mapped: orig_frame/xtc_len = %d/%d = %f\\n'%(xtc_frame,xtc_len,xtc_frame/xtc_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_restraint_distances():\n",
    "    \n",
    "    \"\"\"Computes a umbrella restraint distances and \n",
    "       saves a numpy.savetxt(shape=(num_conf_states(frames),3)) \n",
    "       for each run, saving clone and timestep and distance for each frame.\"\"\"\n",
    "    \n",
    "\n",
    "    print('\\tComputing restraint distances...')\n",
    "    distances_path = project_title + '/umbrella_distances'\n",
    "    if not os.path.exists(distances_path):\n",
    "        os.mkdir(distances_path)\n",
    "\n",
    "    for run in tqdm.tqdm_notebook(range(K)):\n",
    "        distances = []\n",
    "        f = open('%s/run_%d_dists.dat'%(distances_path,run_indices[run]),'a')\n",
    "        f.write('#clone\\ttime(ns)\\t%s\\n'%restrained_distance_labels[run])\n",
    "\n",
    "        traj_files = sorted(glob.glob('%s/traj_data/RUN%d/*xtc'%(project_title,run_indices[run])))\n",
    "\n",
    "        for j in tqdm.tqdm_notebook(range(len(traj_files))):\n",
    "            traj = md.load(traj_files[j], top=structure_file)\n",
    "            ligand_indices = [a.index for a in traj.topology.atoms if a.residue.name in custom_residues]\n",
    "            ligand_COM = compute_center_of_mass(traj,ligand_indices)\n",
    "            protein_xyz = np.take(traj.xyz, [protein_anchor], axis=1)\n",
    "\n",
    "            for k in range(len(traj)):\n",
    "                if k > equil_steps:\n",
    "                    distance_between_groups = np.sqrt((ligand_COM[k][0] - protein_xyz[k][0][0])**2 +\n",
    "                                    (ligand_COM[k][1] - protein_xyz[k][0][1])**2 +\n",
    "                                    (ligand_COM[k][2] - protein_xyz[k][0][2])**2)\n",
    "\n",
    "                    f.write('%d\\t%f\\t%f\\n'%(j,0.1+ 0.1*k,distance_between_groups))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dtraj():\n",
    "    \n",
    "    \"\"\"Computes dtraj object for use in TRAM. Returns and saves a \n",
    "       numpy.ndarray(shape=(num_therm_states(runs), num_conf_states(frames)))\n",
    "       that contains the cluster assignment index for each frame of trajectory data.\"\"\"\n",
    "    \n",
    "    print(\"Computing and saving dtraj object...\")\n",
    "    if not os.path.exists(project_title + '/tram'):\n",
    "        os.mkdir(project_title + '/tram')\n",
    "\n",
    "    #feature_files = []\n",
    "    #for i in range(K): # same order as files were loaded in for computing tICA\n",
    "    #    feature_files += sorted(glob.glob(project_title + '/features/' + \"P*R%d_*npy\"%i))\n",
    "\n",
    "    # load in array of cluster assignments for each frame\n",
    "    assignments_file = glob.glob(project_title + '/tica/' + '*_sequences.npy')[0]\n",
    "    assignments = np.load(assignments_file)\n",
    "    dtraj = np.concatenate(assignments)\n",
    "    #assignments_by_ensemble = []\n",
    "\n",
    "    ## sort cluster assignments based on ensemble\n",
    "    #for i in tqdm.tqdm_notebook(range(K)):\n",
    "    #    assignments_by_ensemble.append([])\n",
    "    #    for j in range(len(feature_files)):\n",
    "    #        if 'R%d_'%i in feature_files[j]:\n",
    "    #            assignments_by_ensemble[i].append(assignments[j])\n",
    "\n",
    "    #dtraj = [np.concatenate(assignments_by_ensemble[x]) for x in range(K)]\n",
    "    np.save(project_title + '/tram/dtraj.npy',dtraj)\n",
    "    \n",
    "    return dtraj\n",
    "\n",
    "\n",
    "def compute_btraj():\n",
    "    \n",
    "    \"\"\"Computes the biased energies for each ensemble in every other ensemble. Returns\n",
    "       a and saves a numpy.ndarray(shape=(num_therm_states(runs),num_uncorr_samples(sum_frames)))\"\"\"\n",
    "    \n",
    "    print('Computing and saving btraj object...')\n",
    "    if not os.path.exists(project_title + '/tram'):\n",
    "        os.mkdir(project_title + '/tram')\n",
    "    \n",
    "    btraj = []\n",
    "    infiles = ['%s/umbrella_distances/run_%d_dists.dat'%(project_title,x) for x in range(K)]\n",
    "    distances = [np.loadtxt(x)[equil_steps:][::stride] for x in infiles]\n",
    "\n",
    "    for k in tqdm.tqdm_notebook(range(K)):\n",
    "        for clone in range(clones):\n",
    "            if not glob.glob(project_title + '/features/P*R%d_C%d.npy'%(k,clone)):\n",
    "                print('Excluding trajectory: %s_R%d_C%d.xtc'%(project_title,k,clone))\n",
    "                continue\n",
    "            frame_reduced_potentials = []\n",
    "            clone_distances = np.asarray([distances[k][x][2] for x in range(len(distances[k])) if distances[k][x][0] == clone])\n",
    "            for l in range(L):\n",
    "                reduced_potential_for_k = beta*(kspring_k[k]/2.0)*(clone_distances - equilibrium_distances[k])**2\n",
    "                reduced_potential_for_l = beta*(kspring_k[l]/2.0)*(clone_distances - equilibrium_distances[l])**2\n",
    "                reduced_potential_for_k_at_l = reduced_potential_for_l - reduced_potential_for_k\n",
    "                frame_reduced_potentials.append(reduced_potential_for_k_at_l)\n",
    "            btraj.append(np.transpose(frame_reduced_potentials))\n",
    "            \n",
    "    np.save(project_title + '/tram/btraj.npy', btraj)\n",
    "    return btraj\n",
    "\n",
    "\n",
    "def compute_ttraj():\n",
    "    \n",
    "    \"\"\"Computes the index of the thermodynamic state (ensemble) for each trajectory frame.\n",
    "       In FAH cases, this is usually just the run. Returns and saves a\n",
    "       numpy.ndarray(shape=(N_traj,num_uncorr_samples(sum_frames/traj)))\"\"\"\n",
    "\n",
    "    print('Computing and saving ttraj object...')\n",
    "    if not os.path.exists(project_title + '/tram'):\n",
    "        os.mkdir(project_title + '/tram')\n",
    "    ttraj = []\n",
    "    \n",
    "    for i in range(runs):\n",
    "        feature_files = sorted(glob.glob(project_title + '/features/' + \"P*R%d_*npy\"%i))\n",
    "        features = [ np.load(filename) for filename in feature_files]\n",
    "        for j in range(len(features)):\n",
    "            ttraj.append([i]*features[j].shape[0])\n",
    "        \n",
    "    np.save(project_title + '/tram/ttraj.npy',ttraj)\n",
    "    \n",
    "    return ttraj\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ukn():\n",
    "    \n",
    "    \"\"\"Computes the biased energies for each ensemble in every other ensemble. Returns\n",
    "       a and saves a numpy.ndarray(shape=(num_therm_states(runs),num_uncorr_samples(sum_frames)))\"\"\"\n",
    "    \n",
    "    print('Computing and saving u_kn object...')\n",
    "    if not os.path.exists(project_title + '/mbar'):\n",
    "        os.mkdir(project_title + '/mbar')\n",
    "    \n",
    "    no_equilibrium_samples = True\n",
    "    \n",
    "    u_kn = []\n",
    "    infiles = ['%s/umbrella_distances/run_%d_dists.dat'%(project_title,x) for x in range(K)]\n",
    "    distances = [np.loadtxt(x)[equil_steps:][::stride] for x in infiles]\n",
    "    N_k = [x.shape[0] for x in distances]\n",
    "    \n",
    "    for k in tqdm.tqdm_notebook(range(K)):\n",
    "        u_kn.append([])\n",
    "        \n",
    "        for l in range(L):\n",
    "            if k == l:\n",
    "                for m in range(len(distances[k])):\n",
    "                    u_kn[k].append(0)\n",
    "\n",
    "            else:\n",
    "                reduced_potential_for_k = beta*(kspring_k[k]/2.0)*(distances[l][:,2] - equilibrium_distances[k])**2\n",
    "                reduced_potential_for_l = beta*(kspring_k[l]/2.0)*(distances[l][:,2] - equilibrium_distances[l])**2\n",
    "                reduced_potential_for_k_at_l = reduced_potential_for_l - reduced_potential_for_k\n",
    "                \n",
    "                for m in reduced_potential_for_k_at_l:\n",
    "                    u_kn[k].append(m)\n",
    "    \n",
    "    if no_equilibrium_samples:\n",
    "        N_k.append(0)\n",
    "        u_kn.append([0]*np.sum(N_k))\n",
    "    \n",
    "    u_kn = np.asarray(u_kn)\n",
    "    np.save(project_title + '/mbar/u_kn.npy', u_kn)\n",
    "    return u_kn, N_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtraj:  (132921,)\n",
      "dtraj[0] 56\n",
      "\n",
      "ttraj:  (922,)\n",
      "np.shape(ttraj[0]) (157,)\n",
      "\n",
      "btraj:  (922,)\n",
      "np.shape(btraj[0]) (156, 20)\n"
     ]
    }
   ],
   "source": [
    "#labels = compute_distances()\n",
    "#eigenvectors = compute_tica_components()\n",
    "#compute_restraint_distances()\n",
    "\n",
    "## TRAM/PyEMMA Calculations\n",
    "\n",
    "#dtraj = compute_dtraj()\n",
    "dtraj = np.load(project_title + '/tram/dtraj.npy')\n",
    "print('dtraj: ',np.shape(dtraj))\n",
    "print('dtraj[0]',dtraj[0])\n",
    "\n",
    "#ttraj = compute_ttraj()\n",
    "ttraj = np.load(project_title + '/tram/ttraj.npy')\n",
    "print('\\nttraj: ',np.shape(ttraj))\n",
    "print('np.shape(ttraj[0])',np.shape(ttraj[0]))\n",
    "\n",
    "#btraj = compute_btraj()\n",
    "btraj = np.load(project_title + '/tram/btraj.npy')\n",
    "print('\\nbtraj: ',np.shape(btraj))\n",
    "print('np.shape(btraj[0])',np.shape(btraj[0]))\n",
    "\n",
    "#mbar_obj = mbar(ttraj, dtraj, btraj)\n",
    "#np.save('stationary_distribution.npy', mbar_obj.stationary_distribution)\n",
    "#np.save('active_set.npy', mbar_obj.active_set)\n",
    "#np.save('free_energies.npy', mbar_obj.free_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K (total states) = 21, total samples = 137604\n",
      "N_k = \n",
      "[6917 6450 7230 7359 7052 7166 7027 6917 6990 6735 6852 6738 6852 6892 6237\n",
      " 7090 6871 7155 6711 6363    0]\n",
      "There are 20 states with samples.\n",
      "Initializing free energies to zero.\n",
      "Initial dimensionless free energies with method zeros\n",
      "f_k = \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n",
      "Final dimensionless free energies\n",
      "f_k = \n",
      "[  0.          13.96688891  26.23346079  36.88345144  45.95602291\n",
      "  53.38560619  59.11469852  63.26850613  66.00361937  67.39679509\n",
      "  67.49070557  66.33976738  64.01382604  60.57926592  56.08753269\n",
      "  50.57681633  44.07703701  36.61032038  28.18335286  18.76481094\n",
      "  43.003483  ]\n",
      "MBAR initialization complete.\n"
     ]
    }
   ],
   "source": [
    "u_kn, N_k = compute_ukn()\n",
    "mbar = MBAR(u_kn,N_k,verbose=True)\n",
    "bin_min = 0.0\n",
    "bin_max = 1.0 # these are distances?\n",
    "dx=(bin_max-bin_min)/n_clusters\n",
    "pmf_distances=np.arange(bin_min,bin_max+dx,dx)\n",
    "nbins = len(pmf_distances)\n",
    "########################################################\n",
    "distance = np.load('distance_99_100.npy')\n",
    "dis = []\n",
    "for i in range(len(distance)):\n",
    "    for j in distance[i]:\n",
    "        dis.append(j)\n",
    "dis = np.array(dis)\n",
    "reshape_dis = dis.reshape(1,u_kn.shape[1])\n",
    "K = u_kn.shape[0]\n",
    "nsnaps = distance.shape[-1]\n",
    "N_k = np.array(K*[nsnaps])\n",
    "u_kn -= u_kn.min()\n",
    "bin_kn = np.zeros(reshape_dis.shape,np.int64)\n",
    "G=[]\n",
    "mbar = MBAR(u_kn, N_k, verbose=True)\n",
    "for j in range(2):\n",
    "    print j\n",
    "    (f_i, df_i) = mbar.computePMF(u_kn[j], bin_kn, nbins)\n",
    "    G.append(f_i)\n",
    "    print f_i, df_i\n",
    "    fout = open('mbar_%d_sub.dat'%(j),'w')\n",
    "    fout.write('# distance (nm)\\tf (kT)\\tdf (kT)\\n')\n",
    "    for i in range(nbins):\n",
    "        fout.write(\"%8.4f\\t%8.4f\\t%8.4f\\n\" % (pmf_distances[i], f_i[i], df_i[i]) )\n",
    "    fout.close()\n",
    "    print 'Wrote'\n",
    "np.save('raw_G.npy',G)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN\tAvgSteps\tEquilSteps\tStatIneff\tnUncSamples\tAutocorrelation\n",
      "0\t3458\t\t2.979\t\t1.054\t\t46\t\t1.049\n",
      "1\t3225\t\t0.340\t\t1.032\t\t49\t\t1.027\n",
      "2\t3615\t\t1.926\t\t1.218\t\t42\t\t1.272\n",
      "3\t3679\t\t2.691\t\t1.051\t\t46\t\t1.016\n",
      "4\t3526\t\t1.213\t\t1.157\t\t45\t\t1.107\n",
      "5\t3583\t\t3.957\t\t1.083\t\t44\t\t1.138\n",
      "6\t3513\t\t2.043\t\t1.089\t\t46\t\t1.161\n",
      "7\t3458\t\t4.309\t\t1.175\t\t41\t\t1.361\n",
      "8\t3495\t\t11.511\t\t1.255\t\t33\t\t1.353\n",
      "9\t3367\t\t4.649\t\t1.282\t\t38\t\t1.206\n",
      "10\t3426\t\t14.489\t\t1.571\t\t25\t\t1.724\n",
      "11\t3369\t\t1.723\t\t1.095\t\t46\t\t1.091\n",
      "12\t3426\t\t2.266\t\t1.162\t\t43\t\t1.056\n",
      "13\t3446\t\t4.170\t\t1.081\t\t44\t\t1.336\n",
      "14\t3118\t\t3.521\t\t1.251\t\t39\t\t1.113\n",
      "15\t3545\t\t7.394\t\t1.446\t\t33\t\t1.731\n",
      "16\t3435\t\t4.723\t\t1.385\t\t37\t\t1.192\n",
      "17\t3577\t\t2.181\t\t1.153\t\t44\t\t1.120\n",
      "18\t3355\t\t3.468\t\t1.319\t\t39\t\t1.283\n",
      "19\t3181\t\t4.085\t\t1.236\t\t40\t\t1.111\n"
     ]
    }
   ],
   "source": [
    "################ subsample / autocorrelation #################\n",
    "\n",
    "from pymbar.timeseries import detectEquilibration\n",
    "from pymbar.timeseries import statisticalInefficiency\n",
    "\n",
    "#%ls tram/umbrella_distances/\n",
    "\n",
    "print('RUN\\tAvgSteps\\tEquilSteps\\tStatIneff\\tnUncSamples\\tAutocorrelation')\n",
    "for k in range(K):\n",
    "    t_k, g_k, Nunc_k, autocorr = [],[],[],[]\n",
    "    k_distances = np.loadtxt('tram/umbrella_distances/run_%d_dists.dat'%k)\n",
    "    for l in range(100):\n",
    "        A_t = np.asarray([x[2] for x in k_distances if x[1] == l])\n",
    "        if A_t.size != 0:\n",
    "            autocorr.append(statisticalInefficiency(A_t,A_t))\n",
    "            indices = detectEquilibration(A_t) # [t, g, Neff_max]\n",
    "            t_k.append(indices[0])\n",
    "            g_k.append(indices[1])\n",
    "            Nunc_k.append(indices[2])\n",
    "    \n",
    "    print('%d\\t%d\\t\\t%.3f\\t\\t%.3f\\t\\t%d\\t\\t%.3f'%(k,len(k_distances)/100,np.mean(t_k),np.mean(g_k),np.mean(Nunc_k),np.mean(autocorr)))\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMRQ for state decomposition\n",
    "# states excluded because of ergotic trimming (if i > j but j !> i, j gets lumped to i)\n",
    "# d tram can be faster if you calculate free energies first with MBAR and use those as biases\n",
    "# use state index as bin index for pmf, you will have a pmf for each ensemble, the bias is the pmf\n",
    "# pmf === free energies\n",
    "#vpyemma has a built-in function for bias with estimate_umbrella_sampling\n",
    "# don't add extra sample, add extra ensemble with no bias for K=k\n",
    "# for its error bars do split into a few clones at a time (same amounts of data tho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
